{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPimu7bO4H2yDbs/ankzlTC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kretchmar/CS339_2023/blob/main/CoinFlipGame_MC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving Coin Flip Game using Monte Carlo\n",
        "\n",
        "## Game Description\n",
        "\n",
        "A player has three turns.  During each turn they can flip a coin or pass.  They accumulate money in their pot during each turn.  At the end of three turns, they get to keep the money in the pot.  \n",
        "\n",
        "- If they \"pass\" during a turn, then the money that is currently in the pot remains there into the next turn. \n",
        "- If they \"flip\" during a turn, then a random 50/50 coin is flipped.  If it lands heads, then 1 dollar is added to the pot.  If it lands tails, then all the money is removed from the pot.  \n",
        "\n",
        "\n",
        "Our goal is to figure out how to flip/pass at each turn to maximize our average winnings.  \n",
        "\n",
        "## State\n",
        "The time steps are $k = \\{0,1,2,3\\}$.   During each time step $x_k$ is the amount of money in the pot which can range from 0 to 3.    We let state $s$ be a function\n",
        "\n",
        "$\n",
        "s = k*4 + x_k\n",
        "$\n",
        "\n",
        "Thus there are 16 states even though some of them are unrealizable in the game (for instance state $s=2$ is $k=0$ and $x_0 = 2$ which is impossible since we can't start the game with 2 dollars in the pot).  \n",
        "## Equations\n",
        "\n",
        "We let $v[(k,x)]$ represent the value of being in state $x$ at turn $k$; that is, with $x$ dollars in the pot at turn $k$.   The value $v[0,0]$ is the expected value we can earn in this game starting at the start state with 0 dollars in the pot.  That is the ultimate value we seek.\n",
        "\n",
        "Let $Q[(k,x),a]$ be the action-value function.  The state we are in is state $(k,x)$ and the action is $a$.   This is the expected value of starting in this state and taking this action.   \n",
        "\n",
        "Let $P[(k,x)]$ be the policy -- the best action choice at state $(k,x)$.   We let 0=pass and 1=flip.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a28oxR8ZUIRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "J83ALCN_UCxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Monte Carlo and System Functions"
      ],
      "metadata": {
        "id": "kVtWTos--RdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------------------------------------------------------------\n",
        "def update (s,a):\n",
        "  '''\n",
        "  Compute next state given state s and action a\n",
        "  s = number of dollars in pot\n",
        "  a = 0 means pass (s' = s)\n",
        "  a = 1 means flip (s' = flip outcome{0,s+1})\n",
        "  return reward (0 for all trajectories)\n",
        "  '''\n",
        "  turn = s // 4 + 1\n",
        "  pot = s % 4\n",
        "\n",
        "  if a == 0:\n",
        "    return pot + turn*4,0\n",
        "\n",
        "  #flip = np.random.randint(1,3)\n",
        "  #if flip == 1:\n",
        "  if np.random.random() < 0.5:\n",
        "    return turn*4,0      # tails\n",
        "  else:\n",
        "    return pot+1+turn*4,0    # heads\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def e_greedy(s,policy,epsilon):\n",
        "  '''\n",
        "  Implements an e-greedy policy.\n",
        "  With probability epsilon, it returns random action choice\n",
        "  otherwise returns action choice specified by the policy\n",
        "\n",
        "  s = current state\n",
        "  policy = policy function (an array that is indexed by state)\n",
        "  epsilon (0 to 1) a probability of picking exploratory random action\n",
        "  '''\n",
        "  r = np.random.random()\n",
        "  if r > epsilon:\n",
        "    return policy[s]\n",
        "  else:\n",
        "    return np.random.randint(0,2)\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def make_trajectory (policy,epsilon):\n",
        "  '''\n",
        "  Simulate one trajectory of experience\n",
        "  Return list of tuples during trajectory\n",
        "  Each tuple is (s,a,r) -> state / action / reward\n",
        "  epsilon = probability of exploratory action\n",
        "  '''\n",
        "  traj = []\n",
        "  k=0\n",
        "  s=0   #  turn 0, pot 0\n",
        "\n",
        "  while (k < 3):\n",
        "    a = e_greedy(s,policy,epsilon)\n",
        "    s_prev = s\n",
        "    s,r = update(s,a)\n",
        "    traj.append((s_prev,a,r))\n",
        "    k += 1\n",
        "\n",
        "  # final reward = state value, final action = 0 (meaningless)\n",
        "  traj.append((s,0,s%4))\n",
        "  return traj\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def init ():\n",
        "  '''\n",
        "  Create totals, counts and policy defaults\n",
        "  '''\n",
        "  totals = np.zeros((16,2))\n",
        "  counts = np.zeros((16,2)).astype(int)\n",
        "  P = np.zeros(16).astype(int)\n",
        "  return totals,counts,P\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_improvement(Q):\n",
        "  '''\n",
        "  Update value function V and policy P based on Q values\n",
        "  '''\n",
        "  V = np.max(Q,axis=1)\n",
        "  P = np.argmax(Q,axis=1)\n",
        "  return V,P\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_evaluation(totals,counts,policy,n,epsilon):\n",
        "  '''\n",
        "  do n trajectories of learning\n",
        "  and update the v/count arrays\n",
        "  '''\n",
        "  for i in range(n):\n",
        "    t = make_trajectory(policy,epsilon)\n",
        "    m = len(t)\n",
        "    #print(m,t)\n",
        "    sum_r = np.zeros(m)\n",
        "    sum_r[m-1] = t[-1][2]\n",
        "    for j in range(m-2,-1,-1):\n",
        "      sum_r[j] = sum_r[j+1] + t[j][2]\n",
        "    #print(sum_r)\n",
        "\n",
        "    for j in range(m):\n",
        "      s,a,r = t[j]\n",
        "      counts[s,a] += 1\n",
        "      totals[s,a] += sum_r[j]\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_iteration(totals,counts,policy,Q,n,m,epsilon):\n",
        "  '''\n",
        "  Perform n iterations of policy iteration \n",
        "  using m trials (episodes) per policy update\n",
        "  '''\n",
        "  for i in range(n):\n",
        "    Q = compute_Q(totals,counts)\n",
        "    V,P = policy_improvement(Q)\n",
        "    policy_evaluation(totals,counts,P,m,epsilon)\n",
        "\n",
        "  print(counts)\n",
        "  Q = compute_Q(totals,counts)\n",
        "  V,P = policy_improvement(Q)\n",
        "  \n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def compute_Q (totals,counts):\n",
        "  '''\n",
        "  Compute the Q values based on totals and counts (average)\n",
        "  '''\n",
        "  Q = np.zeros((16,2))\n",
        "  for i in range(len(totals)):\n",
        "    for a in range(2):\n",
        "      if counts[i][a] > 0:\n",
        "        Q[i][a] = totals[i][a] / counts[i][a]\n",
        "      else:\n",
        "        Q[i][a] = 0\n",
        "  return Q\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def assess (policy,trials):\n",
        "  '''\n",
        "  Assess the value of the current policy by completing #trials\n",
        "  using the specified policy (no e-greedy random actions)\n",
        "  Does not accrue learning experience nor change policy\n",
        "  '''\n",
        "  totals,counts,P = init()\n",
        "  policy_evaluation(totals,counts,policy,trials,0)\n",
        "  Q = compute_Q(totals,counts)\n",
        "  V,P = policy_improvement(Q)\n",
        "  return V[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "ffkp-NgLUT_E"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Trajectory\n",
        "Run a couple of trajectories and test their accuracy"
      ],
      "metadata": {
        "id": "ihFXVWsB_4jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#np.random.seed(seed=42)\n",
        "epsilon = 0.1\n",
        "print('P =',P)\n",
        "sumt = 0\n",
        "k = 10\n",
        "for i in range(k):\n",
        "  t = make_trajectory(P,epsilon)\n",
        "  sumt += t[-1][-1]\n",
        "  if k <= 10:\n",
        "    print(t)\n",
        "\n",
        "print(\"Average Game Value:\",sumt/k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIAKB5YHXMqO",
        "outputId": "b7a282a0-6d5a-476a-9074-fe3b2f8fb7f9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P = [1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0]\n",
            "[(0, 1, 0), (4, 1, 0), (8, 1, 0), (12, 0, 0)]\n",
            "[(0, 1, 0), (4, 1, 0), (9, 1, 0), (12, 0, 0)]\n",
            "[(0, 0, 0), (4, 1, 0), (8, 1, 0), (13, 0, 1)]\n",
            "[(0, 1, 0), (4, 1, 0), (9, 1, 0), (14, 0, 2)]\n",
            "[(0, 1, 0), (5, 1, 0), (10, 0, 0), (14, 0, 2)]\n",
            "[(0, 0, 0), (4, 1, 0), (8, 1, 0), (12, 0, 0)]\n",
            "[(0, 1, 0), (4, 0, 0), (8, 1, 0), (12, 0, 0)]\n",
            "[(0, 1, 0), (4, 1, 0), (8, 1, 0), (12, 0, 0)]\n",
            "[(0, 1, 0), (4, 1, 0), (8, 1, 0), (12, 0, 0)]\n",
            "[(0, 1, 0), (4, 1, 0), (8, 1, 0), (12, 0, 0)]\n",
            "Average Game Value: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do MC Learning\n",
        "This next block does a real segment of MC learning\n",
        "- Start with initial (blank) learning experience\n",
        "- Do 1000 iterations of Policy Iteration each with 100 trials of experience\n",
        "- Extract policy, value function and Q values"
      ],
      "metadata": {
        "id": "4tESNxCyAVGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "totals,counts,P = init()\n",
        "m = 100\n",
        "n = 1000\n",
        "epsilon = 0.1\n",
        "Q = compute_Q(totals,counts)\n",
        "V,P = policy_improvement(Q)\n",
        "policy_iteration(totals,counts,P,Q,n,m,epsilon)\n",
        "\n",
        "Q = compute_Q(totals,counts)\n",
        "V,P = policy_improvement(Q)\n",
        "print('Q =\\n',Q)\n",
        "print('V =\\n',V)\n",
        "print('P =\\n',P)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCBEhfY6UzqA",
        "outputId": "c6fff06c-5229-4187-931b-2446ee2682a1"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5113 94887]\n",
            " [    0     0]\n",
            " [    0     0]\n",
            " [    0     0]\n",
            " [ 2661 49928]\n",
            " [ 2412 44999]\n",
            " [    0     0]\n",
            " [    0     0]\n",
            " [ 2639 47741]\n",
            " [25418  1881]\n",
            " [21201  1120]\n",
            " [    0     0]\n",
            " [28111     0]\n",
            " [49220     0]\n",
            " [22121     0]\n",
            " [  548     0]]\n",
            "Q =\n",
            " [[0.71621357 0.96371473]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.46072905 0.73511857]\n",
            " [0.99626866 1.21722705]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.49856517]\n",
            " [1.         0.97820308]\n",
            " [2.         1.46785714]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [1.         0.        ]\n",
            " [2.         0.        ]\n",
            " [3.         0.        ]]\n",
            "V =\n",
            " [0.96371473 0.         0.         0.         0.73511857 1.21722705\n",
            " 0.         0.         0.49856517 1.         2.         0.\n",
            " 0.         1.         2.         3.        ]\n",
            "P =\n",
            " [1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assessment\n",
        "We can also assess the current policy by conduction many non-learning trials."
      ],
      "metadata": {
        "id": "vWyqcli6ApHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value = assess(P,2000)\n",
        "print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBoKmlE9WFm5",
        "outputId": "22b8ce47-add6-4e23-ab9a-af3bcbe5661a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9985\n"
          ]
        }
      ]
    }
  ]
}