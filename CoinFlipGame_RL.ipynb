{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeAYoisGD22E1OP3Sa7Q7X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kretchmar/CS339_2023/blob/main/CoinFlipGame_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving Coin Flip Game using Reinforcement Learning\n",
        "\n",
        "## Game Description\n",
        "\n",
        "A player has three turns.  During each turn they can flip a coin or pass.  They accumulate money in their pot during each turn.  At the end of three turns, they get to keep the money in the pot.  \n",
        "\n",
        "- If they \"pass\" during a turn, then the money that is currently in the pot remains there into the next turn. \n",
        "- If they \"flip\" during a turn, then a random 50/50 coin is flipped.  If it lands heads, then 1 dollar is added to the pot.  If it lands tails, then all the money is removed from the pot.  \n",
        "\n",
        "\n",
        "Our goal is to figure out how to flip/pass at each turn to maximize our average winnings.  \n",
        "\n",
        "## State\n",
        "The time steps are $k = \\{0,1,2,3\\}$.   During each time step $x_k$ is the amount of money in the pot which can range from 0 to 3.    We let state $s$ be a function\n",
        "\n",
        "$\n",
        "s = k*4 + x_k\n",
        "$\n",
        "\n",
        "Thus there are 16 states even though some of them are unrealizable in the game (for instance state $s=2$ is $k=0$ and $x_0 = 2$ which is impossible since we can't start the game with 2 dollars in the pot).  \n",
        "## Equations\n",
        "\n",
        "We let $v[(k,x)]$ represent the value of being in state $x$ at turn $k$; that is, with $x$ dollars in the pot at turn $k$.   The value $v[0,0]$ is the expected value we can earn in this game starting at the start state with 0 dollars in the pot.  That is the ultimate value we seek.\n",
        "\n",
        "Let $Q[(k,x),a]$ be the action-value function.  The state we are in is state $(k,x)$ and the action is $a$.   This is the expected value of starting in this state and taking this action.   \n",
        "\n",
        "Let $P[(k,x)]$ be the policy -- the best action choice at state $(k,x)$.   We let 0=pass and 1=flip.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a28oxR8ZUIRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J83ALCN_UCxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Reinforcement Learning and System Functions"
      ],
      "metadata": {
        "id": "kVtWTos--RdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-------------------------------------------------------------\n",
        "def update (s,a):\n",
        "  '''\n",
        "  Compute next state given state s and action a\n",
        "  s = number of dollars in pot\n",
        "  a = 0 means pass (s' = s)\n",
        "  a = 1 means flip (s' = flip outcome{0,s+1})\n",
        "  return reward (0 for all trajectories)\n",
        "  '''\n",
        "  turn = s // 4 + 1\n",
        "  pot = s % 4\n",
        "\n",
        "  if a == 0:\n",
        "    return pot + turn*4,0\n",
        "\n",
        "  if np.random.random() < 0.5:\n",
        "    return turn*4,0      # tails\n",
        "  else:\n",
        "    return pot+1+turn*4,0    # heads\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def e_greedy(s,policy,epsilon):\n",
        "  '''\n",
        "  Implements an e-greedy policy.\n",
        "  With probability epsilon, it returns random action choice\n",
        "  otherwise returns action choice specified by the policy\n",
        "\n",
        "  s = current state\n",
        "  policy = policy function (an array that is indexed by state)\n",
        "  epsilon (0 to 1) a probability of picking exploratory random action\n",
        "  '''\n",
        "  r = np.random.random()\n",
        "  if r > epsilon:\n",
        "    return policy[s]\n",
        "  else:\n",
        "    return np.random.randint(0,2)\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def init ():\n",
        "  '''\n",
        "  Create totals, counts and policy defaults\n",
        "  '''\n",
        "  Q = np.zeros((16,2))\n",
        "  P = np.ones(16).astype(int)\n",
        "  return Q,P\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def SARSA (Q,policy,alpha,epsilon):\n",
        "  '''\n",
        "  Perform 1 unit of experience (1 trial, trajectory)\n",
        "  using the SARSA learning algorithm\n",
        "  '''\n",
        "  k=0\n",
        "  s=0   #  turn 0, pot 0\n",
        "  state = k*4+s\n",
        "  action = e_greedy(state,policy,epsilon)\n",
        "  total_reward = 0\n",
        "  reward = 0\n",
        "\n",
        "  #print(\"\\n==== TRIAL ====\")\n",
        "  #print(\"state,action,reward: ({0:d},{1:d},{2:d})\".format(state,action,reward))\n",
        "\n",
        "  while (k < 3):\n",
        "    next_state,reward = update(state,action)\n",
        "    total_reward += reward\n",
        "    next_action = e_greedy(next_state,policy,epsilon)\n",
        "    TDerror = reward + Q[next_state,next_action] - Q[state,action]\n",
        "    Q[state,action] = Q[state,action] + alpha * TDerror\n",
        "    state = next_state\n",
        "    action = next_action\n",
        "    #print(\"state,action,reward: ({0:d},{1:d},{2:d})\".format(state,action,reward))\n",
        "    k += 1\n",
        "\n",
        "  # now we need to update last (terminal) state  \n",
        "  reward = state % 4    # reward is amount in pot\n",
        "  #print(\"state,action,reward: ({0:d},{1:d},{2:d})\".format(state,action,reward))\n",
        "  total_reward += reward\n",
        "  TDerror = reward + 0 - Q[state,action]\n",
        "  Q[state,action] = Q[state,action] + alpha * TDerror\n",
        "  return total_reward\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def QLearning (Q,policy,alpha,epsilon):\n",
        "  '''\n",
        "  Perform 1 unit of experience (1 trial, trajectory)\n",
        "  using the QLearning learning algorithm\n",
        "  '''\n",
        "  k=0\n",
        "  s=0   #  turn 0, pot 0\n",
        "  state = k*4+s\n",
        "  total_reward = 0\n",
        "  reward = 0\n",
        "\n",
        "  #print(\"\\n==== TRIAL ====\")\n",
        "  #print(\"state,action,reward: ({0:d},{1:d},{2:d})\".format(state,action,reward))\n",
        "\n",
        "  while (k < 3):\n",
        "    action = e_greedy(state,policy,epsilon)\n",
        "    next_state,reward = update(state,action)\n",
        "    total_reward += reward\n",
        "\n",
        "    optimal_action = policy[next_state]\n",
        "    TDerror = reward + Q[next_state,optimal_action] - Q[state,action]\n",
        "    Q[state,action] = Q[state,action] + alpha * TDerror\n",
        "    state = next_state\n",
        "    #print(\"state,action,reward: ({0:d},{1:d},{2:d})\".format(state,action,reward))\n",
        "    k += 1\n",
        "\n",
        "  # now we need to update last (terminal) state  \n",
        "  reward = state % 4    # reward is amount in pot\n",
        "  #print(\"state,action,reward: ({0:d},{1:d},{2:d})\".format(state,action,reward))\n",
        "  total_reward += reward\n",
        "  TDerror = reward + 0 - Q[state,action]\n",
        "  Q[state,action] = Q[state,action] + alpha * TDerror\n",
        "  return total_reward\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_improvement(Q):\n",
        "  '''\n",
        "  Update value function V and policy P based on Q values\n",
        "  '''\n",
        "  V = np.max(Q,axis=1)\n",
        "  P = np.argmax(Q,axis=1)\n",
        "  return V,P\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def do_trials (Q,policy,n,alpha,epsilon):\n",
        "  '''\n",
        "  Perform n trials of learning \n",
        "  '''\n",
        "  R = 0   # total reward\n",
        "  for i in range(n):\n",
        "    R += SARSA(Q,policy,alpha,epsilon)\n",
        "\n",
        "  return R / n  \n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def do_QL_trials (Q,policy,n,alpha,epsilon):\n",
        "  '''\n",
        "  Perform n trials of learning \n",
        "  '''\n",
        "  R = 0   # total reward\n",
        "  for i in range(n):\n",
        "    R += QLearning(Q,policy,alpha,epsilon)\n",
        "\n",
        "  return R / n  \n",
        "\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def assess (policy,trials):\n",
        "  '''\n",
        "  Assess the value of the current policy by completing #trials\n",
        "  using the specified policy (no e-greedy random actions)\n",
        "  Does not accrue learning experience nor change policy\n",
        "  '''\n",
        "  R = 0\n",
        "  for i in range(trials):\n",
        "  R += \n",
        "  policy_evaluation(totals,counts,policy,trials,0)\n",
        "  Q = compute_Q(totals,counts)\n",
        "  V,P = policy_improvement(Q)\n",
        "  return V[0]\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def play_game (policy):\n",
        "  '''\n",
        "  Simulate one trajectory of experience\n",
        "  Return list of tuples during trajectory\n",
        "  Each tuple is (s,a,r) -> state / action / reward\n",
        "  epsilon = probability of exploratory action\n",
        "  '''\n",
        "  k=0\n",
        "  s=0   #  turn 0, pot 0\n",
        "  reward = 0\n",
        "\n",
        "  while (k < 3):\n",
        "    a = policy[s]\n",
        "    s,r = update(s,a)\n",
        "    k += 1\n",
        "    reward += r\n",
        "\n",
        "  reward += s % 4\n",
        "  # final reward = state value, final action = 0 (meaningless)\n",
        "  return reward\n"
      ],
      "metadata": {
        "id": "ffkp-NgLUT_E"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do SARSA Learning\n",
        "This next block does a real segment of SARSA learning\n",
        "- Start with initial (blank) learning experience\n",
        "- Do 10 iterations of 1000 trials of SARSA, after each iteration update Policy\n",
        "- Extract policy, value function and Q values"
      ],
      "metadata": {
        "id": "4tESNxCyAVGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q,P = init()\n",
        "m = 10\n",
        "n = 100\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "\n",
        "for i in range(m):\n",
        "  print(\"\\n*** Trial {0:d} ***\".format(i))\n",
        "  R = do_trials(Q,P,n,alpha,epsilon)\n",
        "  V,P = policy_improvement(Q)\n",
        "  print(\"Perf: \",R)\n",
        "\n",
        "print(Q)\n",
        "print(V)\n",
        "print(P)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCBEhfY6UzqA",
        "outputId": "0ee0e929-c9b5-404a-d1fc-9612f8e6be53"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** Trial 0 ***\n",
            "Perf:  0.72\n",
            "\n",
            "*** Trial 1 ***\n",
            "Perf:  1.01\n",
            "\n",
            "*** Trial 2 ***\n",
            "Perf:  1.04\n",
            "\n",
            "*** Trial 3 ***\n",
            "Perf:  0.93\n",
            "\n",
            "*** Trial 4 ***\n",
            "Perf:  0.94\n",
            "\n",
            "*** Trial 5 ***\n",
            "Perf:  0.98\n",
            "\n",
            "*** Trial 6 ***\n",
            "Perf:  0.91\n",
            "\n",
            "*** Trial 7 ***\n",
            "Perf:  1.04\n",
            "\n",
            "*** Trial 8 ***\n",
            "Perf:  0.96\n",
            "\n",
            "*** Trial 9 ***\n",
            "Perf:  0.96\n",
            "[[0.62710214 0.92007939]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.43898676 0.5908592 ]\n",
            " [0.84296694 1.03989386]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.4552618 ]\n",
            " [0.45942847 0.78402406]\n",
            " [1.99035462 0.89143127]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.71757046 1.        ]\n",
            " [1.72982966 2.        ]\n",
            " [0.         2.15271139]]\n",
            "[0.92007939 0.         0.         0.         0.5908592  1.03989386\n",
            " 0.         0.         0.4552618  0.78402406 1.99035462 0.\n",
            " 0.         1.         2.         2.15271139]\n",
            "[1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assessment\n",
        "We can also assess the current policy by conduction many non-learning trials."
      ],
      "metadata": {
        "id": "vWyqcli6ApHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value = assess(P,2000)\n",
        "print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "pBoKmlE9WFm5",
        "outputId": "14c50d5d-1813-40d5-b984-b32f6d8845be"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-9f604dd2c224>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-c4f8889e2e09>\u001b[0m in \u001b[0;36massess\u001b[0;34m(policy, trials)\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0mDoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maccrue\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mexperience\u001b[0m \u001b[0mnor\u001b[0m \u001b[0mchange\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   '''\n\u001b[0;32m--> 157\u001b[0;31m   \u001b[0mtotals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do Q Learning\n",
        "This next block does a real segment of Q learning\n",
        "- Start with initial (blank) learning experience\n",
        "- Do 10 iterations of 1000 trials of Q Learning, after each iteration update Policy\n",
        "- Extract policy, value function and Q values"
      ],
      "metadata": {
        "id": "An8H2uL6VULr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q,P = init()\n",
        "m = 10\n",
        "n = 100\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "\n",
        "for i in range(m):\n",
        "  print(\"\\n*** Trial {0:d} ***\".format(i))\n",
        "  R = do_QL_trials(Q,P,n,alpha,epsilon)\n",
        "  V,P = policy_improvement(Q)\n",
        "  print(\"Perf: \",R)\n",
        "\n",
        "print(Q)\n",
        "print(V)\n",
        "print(P)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_P6adDcVbXE",
        "outputId": "68afdc53-90c0-4d46-aa4d-a12d9c335c34"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** Trial 0 ***\n",
            "Perf:  1.14\n",
            "\n",
            "*** Trial 1 ***\n",
            "Perf:  1.05\n",
            "\n",
            "*** Trial 2 ***\n",
            "Perf:  0.85\n",
            "\n",
            "*** Trial 3 ***\n",
            "Perf:  0.86\n",
            "\n",
            "*** Trial 4 ***\n",
            "Perf:  0.97\n",
            "\n",
            "*** Trial 5 ***\n",
            "Perf:  0.88\n",
            "\n",
            "*** Trial 6 ***\n",
            "Perf:  1.02\n",
            "\n",
            "*** Trial 7 ***\n",
            "Perf:  0.89\n",
            "\n",
            "*** Trial 8 ***\n",
            "Perf:  1.0\n",
            "\n",
            "*** Trial 9 ***\n",
            "Perf:  1.05\n",
            "[[0.74995479 1.07427487]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.49506624 0.83722155]\n",
            " [1.0025149  1.39899835]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.50342282]\n",
            " [0.68118056 1.21495614]\n",
            " [1.99999621 1.18740748]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]\n",
            " [0.74581342 1.        ]\n",
            " [1.99999921 1.99999913]\n",
            " [0.         2.99087024]]\n",
            "[1.07427487 0.         0.         0.         0.83722155 1.39899835\n",
            " 0.         0.         0.50342282 1.21495614 1.99999621 0.\n",
            " 0.         1.         1.99999921 2.99087024]\n",
            "[1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assessment\n",
        "We can also assess the current policy by conduction many non-learning trials."
      ],
      "metadata": {
        "id": "kK2OsEuxV27G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value = assess(P,2000)\n",
        "print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "9voPanxdV4Ye",
        "outputId": "8e3c6126-b309-4443-d2a2-3483fe57005e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-9f604dd2c224>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-c4f8889e2e09>\u001b[0m in \u001b[0;36massess\u001b[0;34m(policy, trials)\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0mDoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maccrue\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mexperience\u001b[0m \u001b[0mnor\u001b[0m \u001b[0mchange\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   '''\n\u001b[0;32m--> 157\u001b[0;31m   \u001b[0mtotals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    }
  ]
}